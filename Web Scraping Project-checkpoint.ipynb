{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdb9b174",
   "metadata": {},
   "source": [
    "# WEB SCRAPING PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65734d9",
   "metadata": {},
   "source": [
    "### Web scraping is the process of extracting and parsing data from websites in an automated fashion using a computer program. It’s a useful technique for creating datasets for research and learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337efb73",
   "metadata": {},
   "source": [
    "# Topic Of Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dab271",
   "metadata": {},
   "source": [
    "# Scraping Top Repositories For Topics On Github"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb10f11",
   "metadata": {},
   "source": [
    "## Introduction:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afa4c96",
   "metadata": {},
   "source": [
    "- We're going to scrape https://github.com/topics\n",
    "- We'll get a list of topics. For each topic, we'll get topic title, topic page URL and topic description\n",
    "- For each topic, we'll get the top 25 repositories in the topic from the topic page\n",
    "- For each repository, we'll grab the repo name, username, stars and repo URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3192fa6",
   "metadata": {},
   "source": [
    "### Pick a website and describe your objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39c3aaf",
   "metadata": {},
   "source": [
    "- We chose the Github's Topic Repositories for it's various topics \n",
    "- We will be extracting the data and parsing it to make it readable\n",
    "- Then we'll remodel the parsed data according to our need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd294ed6",
   "metadata": {},
   "source": [
    "- Github has a lot of topics and a lot more Repositories in those particular topics\n",
    "- It will be tough to extract all the Topics and Repositiories\n",
    "- Therefore, We will be extracting Top Repositories of Top 25 Topics on Github"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b01c22",
   "metadata": {},
   "source": [
    "## Step 1: Install and Import all required Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be3b2bd",
   "metadata": {},
   "source": [
    "### Use the requests library to download web pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547a8a8b",
   "metadata": {},
   "source": [
    "- Request is a library in Python that allows you to send HTTP/1.1 requests extremely easily. There’s no need to manually add query strings to your URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d82505d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3998291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3acf2c3",
   "metadata": {},
   "source": [
    "### Use Beautiful Soup to parse and extract information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71da45f7",
   "metadata": {},
   "source": [
    "- Beautiful Soup is a library that makes it easy to scrape information from web pages. It sits atop an HTML or XML parser, providing Pythonic idioms for iterating, searching, and modifying the parse tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "867ab42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9061a352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c17247",
   "metadata": {},
   "source": [
    "### Use Pandas to save the file as CSV "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96da26ab",
   "metadata": {},
   "source": [
    "- Pandas is an open-source library in Python that is made mainly for working with relational or labeled data both easily and intuitively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec68eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07d782fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258a0226",
   "metadata": {},
   "source": [
    "### Import OS to select the Path where Data will be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6d0d806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be0bbeb",
   "metadata": {},
   "source": [
    "### Getting Topic Informantion From URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e047ae77",
   "metadata": {},
   "source": [
    "- Using the Requests Library to Load the URL\n",
    "- Using Beautiful Soup to Parse the Given URL\n",
    "\n",
    "- Using the Class and Tags from the Parsed URL to Extract the following:\n",
    "    1. Title Of Topics\n",
    "    2. Description Of Topics\n",
    "    3. Url Of Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33656cd",
   "metadata": {},
   "source": [
    "#### Getting Topic Titles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0bcdbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(doc):\n",
    "    selection_class='f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "    topic_title_tags=doc.find_all('p',{'class':selection_class})\n",
    "\n",
    "    topic_titles=[]\n",
    "\n",
    "    for tags in topic_title_tags:\n",
    "        topic_titles.append(tags.text)\n",
    "\n",
    "    return topic_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05224e6",
   "metadata": {},
   "source": [
    "#### Getting Topic Description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91da8b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_descs(doc):\n",
    "    desc_class='f5 color-fg-muted mb-0 mt-1'\n",
    "    topic_desc_tags=doc.find_all('p',{'class':desc_class})\n",
    "\n",
    "    topic_descs=[]\n",
    "\n",
    "    for tags in topic_desc_tags:\n",
    "        topic_descs.append(tags.text.strip())\n",
    "\n",
    "    return topic_descs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8316f07a",
   "metadata": {},
   "source": [
    "#### Getting Topic URL :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87c36ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_url(doc):\n",
    "    topic_link=doc.find_all('a',{'class':'no-underline flex-grow-0'})\n",
    "    len(topic_link)\n",
    "\n",
    "    topic_urls=[]\n",
    "    base_url='https://github.com'\n",
    "    for tags in topic_link:\n",
    "        topic_urls.append(base_url+tags['href'])\n",
    "\n",
    "    return topic_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78261263",
   "metadata": {},
   "source": [
    "### Making Topics Dataframe\n",
    "- Providing The URL link \n",
    "- Getting the Response Object using Requests Library\n",
    "- Using Beautiful Soup to Parse\n",
    "- Calling the other functions to get Required Data\n",
    "- Using pandas to make a Dataframe of the returned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3e2772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic():\n",
    "    topic_url= 'https://github.com/topics'\n",
    "    response= requests.get(topic_url)\n",
    "    doc = BeautifulSoup(response.text,'html.parser')\n",
    "    \n",
    "    topics_dict={'Title':get_topic_titles(doc),'Description':get_topic_descs(doc),'URL':get_topic_url(doc)}\n",
    "\n",
    "    return pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077ce6a2",
   "metadata": {},
   "source": [
    "### Getting Topic Repositories Data\n",
    "- Getting the following items:\n",
    "    1. Repositories Username\n",
    "    2. Repositories Name\n",
    "    3. Repositories URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66453bb7",
   "metadata": {},
   "source": [
    "### Using Topic Dataframe to Parse\n",
    "- Using the Requests Library to Load the Repositories URL\n",
    "- Using Beautiful Soup to Parse the Given URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9952b8",
   "metadata": {},
   "source": [
    "#### Getting Topic Repositiories Page :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a43f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_page(topic_url):\n",
    "    response= requests.get(topic_url)\n",
    "    if response.status_code!=200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    \n",
    "    topic_doc = BeautifulSoup(response.text,'html.parser')\n",
    "    return topic_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37b45f3",
   "metadata": {},
   "source": [
    "#### Getting Repositories Data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8f1af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(repo_tags,repo_stars):\n",
    "    a_tags=repo_tags.find_all('a')\n",
    "    username= a_tags[0].text.strip()\n",
    "    repo_name=a_tags[1].text.strip()\n",
    "    repo_url=base_url+a_tags[1]['href']\n",
    "    stars=int(repo_stars['title'].replace(\",\", \"\"))\n",
    "    return username, repo_name,stars,repo_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517d102d",
   "metadata": {},
   "source": [
    "#### Making a Dataframe of all the Information :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7ab57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_repos(topic_doc):\n",
    "    \n",
    "    \n",
    "    h3_selection_class='f3 color-fg-muted text-normal lh-condensed'\n",
    "    repo_tags=topic_doc.find_all('h3',{'class':h3_selection_class})\n",
    "    \n",
    "    star_selection_class='repo-stars-counter-star'\n",
    "    repo_stars=topic_doc.find_all('span',{'id':star_selection_class})\n",
    "    \n",
    "    repo_dict={'Username':[],'Repo_Name':[],'Stars':[],'Repo_URL':[]}\n",
    "    \n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info=get_repo_info(repo_tags[i],repo_stars[i])\n",
    "        repo_dict['Username'].append(repo_info[0])\n",
    "        repo_dict['Repo_Name'].append(repo_info[1])\n",
    "        repo_dict['Stars'].append(repo_info[2])\n",
    "        repo_dict['Repo_URL'].append(repo_info[3])\n",
    "    return pd.DataFrame(repo_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0810a3f4",
   "metadata": {},
   "source": [
    "#### Calling the functions to make the Dataframe and saving it as CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7215dce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic_repo(topic_urls,path):\n",
    "    if os.path.exists(path):\n",
    "        print('The File {} already exists. Skipping...'.format(path))\n",
    "        return\n",
    "    topic_repo_df=get_topic_repos(get_topic_page(topic_urls))\n",
    "    topic_repo_df.to_csv(path, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297f5613",
   "metadata": {},
   "source": [
    "#### Defining a function to do all the Task in one go :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ac737cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def master_function():\n",
    "    print('Scraping list of topics')\n",
    "    topics_df= scrape_topic()\n",
    "    os.makedirs('Github_Data',exist_ok=True)\n",
    "    for index,row in topics_df.iterrows():\n",
    "        print('Scraping Top Repositories for \"{}\"'.format(row['Title']))\n",
    "        scrape_topic_repo(row['URL'],'Github_Data/{}.csv'.format(row['Title']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dbad3b",
   "metadata": {},
   "source": [
    "### Calling the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "396599d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping list of topics\n",
      "Scraping Top Repositories for \"3D\"\n",
      "The File Github_Data/3D.csv already exists. Skipping...\n",
      "Scraping Top Repositories for \"Ajax\"\n",
      "The File Github_Data/Ajax.csv already exists. Skipping...\n",
      "Scraping Top Repositories for \"Algorithm\"\n",
      "The File Github_Data/Algorithm.csv already exists. Skipping...\n",
      "Scraping Top Repositories for \"Amp\"\n",
      "The File Github_Data/Amp.csv already exists. Skipping...\n",
      "Scraping Top Repositories for \"Android\"\n",
      "The File Github_Data/Android.csv already exists. Skipping...\n",
      "Scraping Top Repositories for \"Angular\"\n",
      "The File Github_Data/Angular.csv already exists. Skipping...\n",
      "Scraping Top Repositories for \"Ansible\"\n",
      "The File Github_Data/Ansible.csv already exists. Skipping...\n",
      "Scraping Top Repositories for \"API\"\n",
      "The File Github_Data/API.csv already exists. Skipping...\n",
      "Scraping Top Repositories for \"Arduino\"\n",
      "The File Github_Data/Arduino.csv already exists. Skipping...\n",
      "Scraping Top Repositories for \"ASP.NET\"\n",
      "The File Github_Data/ASP.NET.csv already exists. Skipping...\n",
      "Scraping Top Repositories for \"Atom\"\n",
      "The File Github_Data/Atom.csv already exists. Skipping...\n",
      "Scraping Top Repositories for \"Awesome Lists\"\n",
      "The File Github_Data/Awesome Lists.csv already exists. Skipping...\n",
      "Scraping Top Repositories for \"Amazon Web Services\"\n",
      "The File Github_Data/Amazon Web Services.csv already exists. Skipping...\n",
      "Scraping Top Repositories for \"Azure\"\n",
      "The File Github_Data/Azure.csv already exists. Skipping...\n",
      "Scraping Top Repositories for \"Babel\"\n",
      "The File Github_Data/Babel.csv already exists. Skipping...\n",
      "Scraping Top Repositories for \"Bash\"\n",
      "The File Github_Data/Bash.csv already exists. Skipping...\n",
      "Scraping Top Repositories for \"Bitcoin\"\n",
      "The File Github_Data/Bitcoin.csv already exists. Skipping...\n",
      "Scraping Top Repositories for \"Bootstrap\"\n",
      "The File Github_Data/Bootstrap.csv already exists. Skipping...\n",
      "Scraping Top Repositories for \"Bot\"\n",
      "The File Github_Data/Bot.csv already exists. Skipping...\n",
      "Scraping Top Repositories for \"C\"\n",
      "The File Github_Data/C.csv already exists. Skipping...\n",
      "Scraping Top Repositories for \"Chrome\"\n",
      "The File Github_Data/Chrome.csv already exists. Skipping...\n",
      "Scraping Top Repositories for \"Chrome extension\"\n",
      "The File Github_Data/Chrome extension.csv already exists. Skipping...\n",
      "Scraping Top Repositories for \"Command line interface\"\n",
      "The File Github_Data/Command line interface.csv already exists. Skipping...\n",
      "Scraping Top Repositories for \"Clojure\"\n",
      "The File Github_Data/Clojure.csv already exists. Skipping...\n",
      "Scraping Top Repositories for \"Code quality\"\n",
      "The File Github_Data/Code quality.csv already exists. Skipping...\n",
      "Scraping Top Repositories for \"Code review\"\n",
      "The File Github_Data/Code review.csv already exists. Skipping...\n",
      "Scraping Top Repositories for \"Compiler\"\n",
      "The File Github_Data/Compiler.csv already exists. Skipping...\n",
      "Scraping Top Repositories for \"Continuous integration\"\n",
      "The File Github_Data/Continuous integration.csv already exists. Skipping...\n",
      "Scraping Top Repositories for \"COVID-19\"\n",
      "The File Github_Data/COVID-19.csv already exists. Skipping...\n",
      "Scraping Top Repositories for \"C++\"\n",
      "The File Github_Data/C++.csv already exists. Skipping...\n"
     ]
    }
   ],
   "source": [
    "master_function()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
